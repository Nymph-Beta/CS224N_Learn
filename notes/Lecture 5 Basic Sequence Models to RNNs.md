## Lecture Notes: Part V  Language Models, RNN, GRU and LSTM

### Language Model
#### 1. 语言模型的定义与近似（Introduction）
语言模型的核心任务是计算一个词序列 $w_1, ..., w_m$ 出现的概率。
- 核心公式 (Equation 1)：
    $$P(w_{1},...,w_{m})=\prod_{i=1}^{i=m}P(w_{i}|w_{1},...,w_{i-1})\approx\prod_{i=1}^{i=m}P(w_{i}|w_{i-n},...,w_{i-1})$$
    - 左半部分（链式法则）：任何序列的联合概率都可以分解为一系列条件概率的乘积。即：第 $i$ 个词出现的概率，取决于它前面所有的词（$w_1$ 到 $w_{i-1}$）。
    - 右半部分（马尔可夫假设/近似）：因为计算“前面所有的词”太复杂且数据稀疏，我们通常假设当前词只依赖于前面 $n$ 个词的窗口。这就是近似符号 $\approx$ 的来源。
    - 应用场景：这主要用于机器翻译或语音识别，用来判断生成的句子是否通顺（即概率高低）。
#### 2. 基于统计的 n-gram 模型（n-gram Language Models）
在深度学习之前，我们通过“数数”来计算上述的条件概率。

- Bigram（二元）公式 (Equation 2)：$$p(w_{2}|w_{1})=\frac{count(w_{1},w_{2})}{count(w_{1})}$$
    - 含义：在 $w_1$ 出现的情况下，紧接着出现 $w_2$ 的概率。
    - 计算：用“$w_1, w_2$ 同时出现的次数”除以“$w_1$ 单独出现的总次数”。
- Trigram（三元）公式 (Equation 3)：$$p(w_{3}|w_{1},w_{2})=\frac{count(w_{1},w_{2},w_{3})}{count(w_{1},w_{2})}$$
    - 含义：基于前两个词 $w_1, w_2$ 来预测第三个词 $w_3$。
    - 问题：这种方法有两个致命缺陷：
        - 稀疏性（Sparsity）：如果分子（三个词从未一起出现）或分母（前两个词从未一起出现）为0，概率无法计算，需要用到平滑（Smoothing）或回退（Backoff）技术 。
        - 存储（Storage）：你需要存储所有见过的 n-gram 的计数，随着 $n$ 增大，模型体积呈指数级爆炸 。
#### 基于窗口的神经语言模型（Window-based Neural Language Model）
为了解决上述“维度灾难”，Bengio 等人提出了神经概率语言模型（Neural Probabilistic Language Model）。它不再存计数表，而是通过神经网络来拟合概率函数。
- 核心架构公式 (Equation 4)：$$\hat{y}=softmax(W^{(2)}tanh(W^{(1)}x+b^{(1)})+W^{(3)}x+b^{(3)})$$
    这个公式描述了整个网络的前向传播过程：
    1. 输入 $x$：
        - 这里 $x$ 代表输入窗口中所有词向量的拼接（concatenation）。如果窗口大小是 $n$，每个词向量维度是 $d$，那么 $x$ 的维度就是 $n \times d$。
    2. 第一部分：隐层（特征提取）
        - $W^{(1)}x + b^{(1)}$：这是一个全连接线性层。$W^{(1)}$ 负责将输入的词向量映射到隐层空间。
        - $tanh(...)$：这是非线性激活函数。
        - $W^{(2)}$：将隐层的特征映射到输出层。
    3. 第二部分：直连通道（Skip Connection）
        - $W^{(3)}x + b^{(3)}$：这是一个从输入直接到输出的线性连接。
        - 意义：这部分允许模型直接利用原始的词向量信息，而不经过非线性变换。这在早期的神经网络设计中很常见，用来加速训练和保护特征。
    4. 输出 $\hat{y}$：
        - $softmax(...)$：将最终的得分转换为概率分布。$\hat{y}$ 是一个向量，长度等于词表大小 $|V|$，其中第 $i$ 个元素表示下一个词是词表中第 $i$ 个词的概率。

    ---
    在输入层（Input Layer）和输出层（Output Layer）之间，所有看不见的、负责提取特征的层，都叫隐层。

    数字上标 $(1)$ 通常代表权重矩阵所属的“变换顺序”或“目标层级”。计数通常是从输入层之后的第一步计算开始算的。
    ```Plaintext
    输入层 (Layer 0)      隐层 (Layer 1)         输出层 (Layer 2)
        x   -----------------> tanh -----------------> softmax
                (过路费 W1)            (过路费 W2)

        |                                            ^
        |--------------------------------------------|
                        (过路费 W3 - 直连)
    ```